{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BERT.pytorch_pretrained_bert.modeling import BertConfig\n",
    "from BERT.pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from BERT.pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from distiller import TaskSpecificDistiller\n",
    "from causal_distiller import TaskSpecificCausalDistiller\n",
    "\n",
    "from src.argument_parser import default_parser, get_predefine_argv, complete_argument\n",
    "from src.nli_data_processing import processors, output_modes\n",
    "from src.data_processing import init_model, get_task_dataloader\n",
    "from src.modeling import BertForSequenceClassificationEncoder, FCClassifierForSequenceClassification, FullFCClassifierForSequenceClassification\n",
    "from src.utils import load_model, count_parameters, eval_model_dataloader_nli, eval_model_dataloader\n",
    "from src.KD_loss import distillation_loss, patience_loss\n",
    "from envs import HOME_DATA_FOLDER\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:38:10 - INFO - __main__ -   IN DEBUG MODE\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   encoder checkpoint not provided, use pre-trained at /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/pretrained/bert-base-uncased/pytorch_model.bin instead\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   encoder checkpoint not provided, use default directory for fine-tuned model at /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin instead\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   encoder checkpoint not provided, use default directory for fine-tuned model at /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin instead\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   folder exist but empty, use it as output\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   device: cpu n_gpu: 0, 16-bits training: False\n",
      "02/11/2022 01:38:10 - INFO - src.argument_parser -   random seed = 95706824\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# Prepare Parser\n",
    "##########################################################################\n",
    "parser = default_parser()\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    logger.info(\"IN DEBUG MODE\")\n",
    "    # run simple fune-tuning *teacher* by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'finetune_teacher')\n",
    "\n",
    "    # run simple fune-tuning *student* by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'finetune_student')\n",
    "\n",
    "    # run vanilla KD by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'kd')\n",
    "\n",
    "    # run Patient Teacher by uncommenting below cmd\n",
    "    argv = get_predefine_argv('glue', 'SST-2', 'kd.cls')\n",
    "    try:\n",
    "        args = parser.parse_args(argv)\n",
    "    except NameError:\n",
    "        raise ValueError('please uncomment one of option above to start training')\n",
    "    args.max_training_examples = 1000\n",
    "else:\n",
    "    logger.info(\"IN CMD MODE\")\n",
    "    args = parser.parse_args()\n",
    "args = complete_argument(args, is_debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:38:10 - INFO - __main__ -   actual batch size on all GPU = 32\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   Input Argument Information\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   task_name                     SST-2\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   run_name                      kd.cls.True_SST-2_nlayer.6_lr.1e-05_T.10.0_alpha.0.7_beta.500.0_bs.32\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   output_dir                    /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/outputs/KD/SST-2/teacher_12layer/kd.cls.True_SST-2_nlayer.6_lr.1e-05_T.10.0_alpha.0.7_beta.500.0_bs.32-run-1\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   log_every_step                1\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   log_interval                  500\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   checkpoint_interval           4000\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   max_seq_length                128\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   max_training_examples         1000\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   seed                          95706824\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   train_batch_size              32\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   eval_batch_size               32\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   learning_rate                 1e-05\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   num_train_epochs              4.0\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   gradient_accumulation_steps   1\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   fp16                          False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   loss_scale                    0\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   student_hidden_layers         6\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   teacher_prediction            /home/JJteam/Project/PatientTeacherforBERT/data/outputs/KD/SST-2/SST-2_patient_kd_teacher_12layer_result_summary.pkl\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   warmup_proportion             0.1\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   bert_model                    /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/pretrained/bert-base-uncased\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   encoder_checkpoint_student    /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/pretrained/bert-base-uncased/pytorch_model.bin\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   cls_checkpoint_student        None\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   encoder_checkpoint_teacher    /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   cls_checkpoint_teacher        /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   output_all_encoded_layers     False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   alpha                         0.7\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   T                             10.0\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   beta                          500.0\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   kd_model                      kd.cls\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   fc_layer_idx                  1,3,5,7,9\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   weights                       None\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   normalize_patience            True\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   do_train                      True\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   do_eval                       True\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   is_diito                      False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   diito_type                    random\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   interchange_prop              0.3\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   interchange_max_token         -1\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   interchange_masked_token_only  False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   interchange_consecutive_only  False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   data_augment                  False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   is_wandb                      False\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   wandb_metadata                \n",
      "02/11/2022 01:38:10 - INFO - __main__ -   device                        cpu\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   n_gpu                         0\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   raw_data_dir                  /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/data_raw/SST-2\n",
      "02/11/2022 01:38:10 - INFO - __main__ -   feat_data_dir                 /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/data_feat/SST-2\n",
      "02/11/2022 01:38:10 - INFO - BERT.pytorch_pretrained_bert.tokenization -   loading vocabulary file /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/pretrained/bert-base-uncased/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "args.raw_data_dir = os.path.join(HOME_DATA_FOLDER, 'data_raw', args.task_name)\n",
    "args.feat_data_dir = os.path.join(HOME_DATA_FOLDER, 'data_feat', args.task_name)\n",
    "\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "logger.info('actual batch size on all GPU = %d' % args.train_batch_size)\n",
    "device, n_gpu = args.device, args.n_gpu\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "logger.info('Input Argument Information')\n",
    "args_dict = vars(args)\n",
    "for a in args_dict:\n",
    "    logger.info('%-28s  %s' % (a, args_dict[a]))\n",
    "\n",
    "#########################################################################\n",
    "# Prepare  Data\n",
    "##########################################################################\n",
    "task_name = args.task_name.lower()\n",
    "\n",
    "if task_name not in processors and 'race' not in task_name:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "if 'race' in task_name:\n",
    "    pass\n",
    "else:\n",
    "    processor = processors[task_name]()\n",
    "    output_mode = output_modes[task_name]\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:38:10 - INFO - __main__ -   skipping loading teacher's predictoin, we calculate this on-the-fly\n",
      "02/11/2022 01:38:11 - INFO - src.nli_data_processing -   Writing example 0 of 1000\n",
      "02/11/2022 01:38:11 - INFO - __main__ -   ***** Running training *****\n",
      "02/11/2022 01:38:11 - INFO - __main__ -     Num examples = 1000\n",
      "02/11/2022 01:38:11 - INFO - __main__ -     Batch size = 32\n",
      "02/11/2022 01:38:11 - INFO - __main__ -     Num steps = 124\n",
      "02/11/2022 01:38:11 - INFO - src.nli_data_processing -   Writing example 0 of 872\n",
      "02/11/2022 01:38:11 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2022 01:38:11 - INFO - __main__ -     Num examples = 872\n",
      "02/11/2022 01:38:11 - INFO - __main__ -     Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    train_sampler = SequentialSampler if DEBUG else RandomSampler\n",
    "    read_set = 'train'\n",
    "    logger.info('skipping loading teacher\\'s predictoin, we calculate this on-the-fly')\n",
    "    train_examples, train_dataloader, _ = get_task_dataloader(task_name, read_set, tokenizer, args, SequentialSampler,\n",
    "                                                              batch_size=args.train_batch_size)\n",
    "    num_train_optimization_steps = int(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    args.num_train_optimization_steps = num_train_optimization_steps\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_examples, eval_dataloader, eval_label_ids = get_task_dataloader(task_name, 'dev', tokenizer, args, SequentialSampler, batch_size=args.eval_batch_size)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:38:11 - INFO - __main__ -   using normal Knowledge Distillation\n",
      "02/11/2022 01:38:11 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:11 - INFO - __main__ -   Loading the student model...\n",
      "02/11/2022 01:38:11 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:11 - INFO - src.nli_data_processing -   predicting for SST-2\n",
      "02/11/2022 01:38:11 - INFO - src.modeling -   num hidden layer is set as 6\n",
      "02/11/2022 01:38:11 - INFO - src.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/11/2022 01:38:13 - INFO - src.utils -   loading BertForSequenceClassificationEncoder finetuned model from /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/pretrained/bert-base-uncased/pytorch_model.bin\n",
      "02/11/2022 01:38:21 - INFO - src.utils -   number of keep_keys=103, number of model_keys=103\n",
      "02/11/2022 01:38:21 - INFO - src.utils -   delete 104 layers, keep 103 layers\n",
      "02/11/2022 01:38:21 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:21 - INFO - src.utils -   no checkpoint provided for FCClassifierForSequenceClassification!\n",
      "02/11/2022 01:38:21 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:21 - INFO - __main__ -   Loading the teacher model...\n",
      "02/11/2022 01:38:21 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:21 - INFO - src.nli_data_processing -   predicting for SST-2\n",
      "02/11/2022 01:38:21 - INFO - src.modeling -   num hidden layer is set as 12\n",
      "02/11/2022 01:38:21 - INFO - src.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/11/2022 01:38:24 - INFO - src.utils -   loading BertForSequenceClassificationEncoder finetuned model from /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin\n",
      "02/11/2022 01:38:24 - INFO - src.utils -   number of keep_keys=199, number of model_keys=199\n",
      "02/11/2022 01:38:25 - INFO - src.utils -   delete 3 layers, keep 199 layers\n",
      "02/11/2022 01:38:25 - INFO - __main__ -   *****************************************************************************\n",
      "02/11/2022 01:38:25 - INFO - src.utils -   loading FCClassifierForSequenceClassification finetuned model from /dfs/user/wuzhengx/workspace/Causal-Distill-XXS/data/models/finetuned/bert-base-uncased/SST-2/pytorch_model.bin\n",
      "02/11/2022 01:38:25 - INFO - src.utils -   number of keep_keys=2, number of model_keys=2\n",
      "02/11/2022 01:38:25 - INFO - src.utils -   delete 200 layers, keep 2 layers\n",
      "02/11/2022 01:38:25 - INFO - __main__ -   number of layers in student model = 6\n",
      "02/11/2022 01:38:25 - INFO - __main__ -   num parameters in student model are 66955008 and 1538\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# Prepare model\n",
    "#########################################################################\n",
    "student_config = BertConfig(os.path.join(args.bert_model, 'bert_config.json'))\n",
    "teacher_config = BertConfig(os.path.join(args.bert_model, 'bert_config.json'))\n",
    "if args.kd_model.lower() in ['kd', 'kd.cls']:\n",
    "    logger.info('using normal Knowledge Distillation')\n",
    "    output_all_layers = args.kd_model.lower() == 'kd.cls'\n",
    "    logger.info('*' * 77)\n",
    "    logger.info(\"Loading the student model...\")\n",
    "    logger.info('*' * 77)\n",
    "    student_encoder, student_classifier = init_model(\n",
    "        task_name, output_all_layers, \n",
    "        args.student_hidden_layers, student_config,\n",
    "    )\n",
    "\n",
    "    n_student_layer = len(student_encoder.bert.encoder.layer)\n",
    "    student_encoder = load_model(\n",
    "        student_encoder, args.encoder_checkpoint_student, args, 'student', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    logger.info('*' * 77)\n",
    "    student_classifier = load_model(\n",
    "        student_classifier, args.cls_checkpoint_student, args, 'classifier', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    \n",
    "    logger.info('*' * 77)\n",
    "    logger.info(\"Loading the teacher model...\")\n",
    "    logger.info('*' * 77)\n",
    "    # since we also calculate teacher's output on-fly, we need to load the teacher model as well.\n",
    "    # note that, we assume teacher model is pre-trained already.\n",
    "    teacher_encoder, teacher_classifier = init_model(\n",
    "        task_name, output_all_layers, \n",
    "        teacher_config.num_hidden_layers, teacher_config,\n",
    "    )\n",
    "    \n",
    "    n_teacher_layer = len(teacher_encoder.bert.encoder.layer)\n",
    "    teacher_encoder = load_model(\n",
    "        teacher_encoder, args.encoder_checkpoint_teacher, args, 'student', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    logger.info('*' * 77)\n",
    "    teacher_classifier = load_model(\n",
    "        teacher_classifier, args.cls_checkpoint_teacher, args, 'classifier', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # originally, the codebase supports kd.full, but that is never used.\n",
    "    raise ValueError('%s KD not found, please use kd or kd.cls' % args.kd)\n",
    "\n",
    "n_param_student = count_parameters(student_encoder) + count_parameters(student_classifier)\n",
    "logger.info('number of layers in student model = %d' % n_student_layer)\n",
    "logger.info('num parameters in student model are %d and %d' % (count_parameters(student_encoder), count_parameters(student_classifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import wandb\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BERT.pytorch_pretrained_bert.modeling import BertConfig\n",
    "from BERT.pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from BERT.pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from src.argument_parser import default_parser, get_predefine_argv, complete_argument\n",
    "from src.nli_data_processing import processors, output_modes\n",
    "from src.data_processing import init_model, get_task_dataloader\n",
    "from src.modeling import BertForSequenceClassificationEncoder, FCClassifierForSequenceClassification, FullFCClassifierForSequenceClassification\n",
    "from src.utils import load_model, count_parameters, eval_model_dataloader_nli, eval_model_dataloader\n",
    "from src.KD_loss import distillation_loss, patience_loss\n",
    "from envs import HOME_DATA_FOLDER\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TaskSpecificCausalDistiller:\n",
    "    def __init__(\n",
    "        self, params, \n",
    "        train_dataset, eval_dataset, \n",
    "        eval_label_ids, num_labels, output_mode,\n",
    "        student_encoder: nn.Module, student_classifier: nn.Module,\n",
    "        teacher_encoder: nn.Module, teacher_classifier: nn.Module,\n",
    "    ):\n",
    "        if params.is_wandb:\n",
    "            run = wandb.init(\n",
    "                project=params.wandb_metadata.split(\":\")[-1], \n",
    "                entity=params.wandb_metadata.split(\":\")[0],\n",
    "                name=params.run_name,\n",
    "            )\n",
    "            wandb.config.update(params)\n",
    "        self.is_wandb = params.is_wandb\n",
    "        logger.info(\"Initializing Normal Distiller (Task Specific)\")\n",
    "        \n",
    "        self.params = params\n",
    "        \n",
    "        self.output_model_file = '{}_nlayer.{}_lr.{}_T.{}.alpha.{}_beta.{}_bs.{}'.format(\n",
    "            self.params.task_name, \n",
    "            self.params.student_hidden_layers,\n",
    "            self.params.learning_rate,\n",
    "            self.params.T, \n",
    "            self.params.alpha, \n",
    "            self.params.beta,\n",
    "            self.params.train_batch_size * self.params.gradient_accumulation_steps\n",
    "        )\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.eval_label_ids = eval_label_ids\n",
    "        self.num_labels = num_labels\n",
    "        self.output_mode = output_mode\n",
    "        \n",
    "        self.student_encoder = student_encoder\n",
    "        self.student_classifier = student_classifier\n",
    "        self.teacher_encoder = teacher_encoder\n",
    "        self.teacher_classifier = teacher_classifier\n",
    "        \n",
    "        # common used vars\n",
    "        self.fp16 = params.fp16\n",
    "        self.T = params.T\n",
    "        self.alpha = params.alpha\n",
    "        self.beta = params.beta\n",
    "        self.normalize_patience = params.normalize_patience\n",
    "        self.learning_rate = params.learning_rate\n",
    "        self.train_batch_size = params.train_batch_size\n",
    "        self.output_dir = params.output_dir\n",
    "        self.warmup_proportion = params.warmup_proportion\n",
    "        self.num_train_optimization_steps = params.num_train_optimization_steps\n",
    "        self.task_name = params.task_name\n",
    "        self.kd_model = params.kd_model \n",
    "        self.weights = params.weights\n",
    "        self.fc_layer_idx = params.fc_layer_idx\n",
    "        self.n_gpu = params.n_gpu\n",
    "        self.device = params.device\n",
    "        self.num_train_epochs = params.num_train_epochs\n",
    "        self.gradient_accumulation_steps = params.gradient_accumulation_steps\n",
    "        self.loss_scale = params.loss_scale\n",
    "        \n",
    "        # DIITO params\n",
    "        self.is_diito = params.is_diito\n",
    "        self.diito_type = params.diito_type\n",
    "        self.interchange_prop = params.interchange_prop\n",
    "        self.interchange_max_token = params.interchange_max_token\n",
    "        self.interchange_masked_token_only = params.interchange_masked_token_only\n",
    "        self.interchange_consecutive_only = params.interchange_consecutive_only\n",
    "        self.data_augment = params.data_augment\n",
    "        \n",
    "        # log to a local file\n",
    "        log_train = open(os.path.join(self.output_dir, 'train_log.txt'), 'w', buffering=1)\n",
    "        log_eval = open(os.path.join(self.output_dir, 'eval_log.txt'), 'w', buffering=1)\n",
    "        print('epoch,global_steps,step,acc,loss,kd_loss,ce_loss,AT_loss', file=log_train)\n",
    "        print('epoch,acc,loss', file=log_eval)\n",
    "        log_train.close()\n",
    "        log_eval.close()\n",
    "    \n",
    "        param_optimizer = list(\n",
    "            self.student_encoder.named_parameters()\n",
    "        ) + list(\n",
    "            self.student_classifier.named_parameters()\n",
    "        )\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        if self.fp16:\n",
    "            logger.info('FP16 activate, use apex FusedAdam')\n",
    "            try:\n",
    "                from apex.optimizers import FP16_Optimizer\n",
    "                from apex.optimizers import FusedAdam\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "            self.optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                                  lr=self.learning_rate,\n",
    "                                  bias_correction=False,\n",
    "                                  max_grad_norm=1.0)\n",
    "            if self.loss_scale == 0:\n",
    "                self.optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "            else:\n",
    "                self.optimizer = FP16_Optimizer(optimizer, static_loss_scale=self.loss_scale)\n",
    "        else:\n",
    "            logger.info('FP16 is not activated, use BertAdam')\n",
    "            self.optimizer = BertAdam(\n",
    "                optimizer_grouped_parameters,\n",
    "                lr=self.learning_rate,\n",
    "                warmup=self.warmup_proportion,\n",
    "                t_total=self.num_train_optimization_steps\n",
    "            )\n",
    "        \n",
    "        # other params that report to tensorboard\n",
    "        self.epoch = 0\n",
    "        self.n_iter = 0\n",
    "        self.n_total_iter = 0\n",
    "        self.n_sequences_epoch = 0\n",
    "        self.total_loss_epoch = 0\n",
    "        self.last_loss = 0\n",
    "        self.last_loss_dl = 0\n",
    "        self.last_kd_loss = 0\n",
    "        self.last_ce_loss = 0 \n",
    "        self.last_pt_loss = 0\n",
    "        self.lr_this_step = 0\n",
    "        self.last_log = 0\n",
    "        \n",
    "        self.acc_tr_loss = 0\n",
    "        self.acc_tr_kd_loss = 0\n",
    "        self.acc_tr_ce_loss = 0\n",
    "        self.acc_tr_pt_loss = 0\n",
    "        self.acc_tr_acc = 0\n",
    "        \n",
    "        self.tr_loss = 0\n",
    "        self.tr_kd_loss = 0\n",
    "        self.tr_ce_loss = 0\n",
    "        self.tr_pt_loss = 0\n",
    "        self.tr_acc = 0\n",
    "        \n",
    "        # DIITO related params that report to tensorboard\n",
    "    \n",
    "    def prepare_batch(self, input_ids, input_mask, segment_ids, label_ids, ):\n",
    "        if self.is_diito:\n",
    "            dual_input_ids = input_ids.clone()\n",
    "            dual_input_mask = input_mask.clone()\n",
    "            dual_segment_ids = segment_ids.clone()\n",
    "            dual_label_ids = label_ids.clone()\n",
    "            causal_sort_index = [i for i in range(dual_input_ids.shape[0])]\n",
    "            random.shuffle(causal_sort_index)\n",
    "            dual_input_ids = dual_input_ids[causal_sort_index]\n",
    "            dual_input_mask = dual_input_mask[causal_sort_index]\n",
    "            dual_segment_ids = dual_segment_ids[causal_sort_index]\n",
    "            dual_label_ids = dual_label_ids[causal_sort_index]\n",
    "            return input_ids, input_mask, segment_ids, label_ids, \\\n",
    "                dual_input_ids, dual_input_mask, dual_segment_ids, dual_label_ids\n",
    "        else:\n",
    "            return input_ids, input_mask, segment_ids, label_ids\n",
    "    \n",
    "    def train(self):\n",
    "        global_step = 0\n",
    "        nb_tr_steps = 0\n",
    "        tr_loss = 0\n",
    "        self.student_encoder.train()\n",
    "        self.student_classifier.train()\n",
    "        self.teacher_encoder.eval()\n",
    "        self.teacher_classifier.eval()\n",
    "        self.last_log = time.time()\n",
    "        \n",
    "        for epoch in trange(int(self.num_train_epochs), desc=\"Epoch\"):\n",
    "            tr_loss, tr_ce_loss, tr_kd_loss, tr_acc = 0, 0, 0, 0\n",
    "            nb_tr_examples, nb_tr_steps = 0, 0\n",
    "            \n",
    "            iter_bar = tqdm(self.train_dataset, desc=\"-Iter\", disable=False)\n",
    "            for batch in iter_bar:\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                # teascher patient is on-the-fly, we can skip the logic for different batch format.\n",
    "                prepared_batch = self.prepare_batch(\n",
    "                    *batch,\n",
    "                )\n",
    "                if self.is_diito:\n",
    "                    self.step_diito(\n",
    "                        *prepared_batch\n",
    "                    )\n",
    "                else:\n",
    "                    self.step(\n",
    "                        *prepared_batch\n",
    "                    )\n",
    "                iter_bar.update()\n",
    "                iter_bar.set_postfix(\n",
    "                    {\n",
    "                        \"Last_loss\": f\"{self.last_loss:.2f}\", \n",
    "                        \"Avg_cum_loss\": f\"{self.total_loss_epoch/self.n_iter:.2f}\", \n",
    "                    }\n",
    "                )\n",
    "            iter_bar.close()\n",
    "\n",
    "            logger.info(f\"--- Ending epoch {self.epoch}/{self.num_train_epochs-1}\")\n",
    "            self.end_epoch()\n",
    "\n",
    "        logger.info(\"Save very last checkpoint as `pytorch_model.bin`.\")\n",
    "        self.save_checkpoint(checkpoint_name=\"pytorch_model.bin\")\n",
    "        logger.info(\"Training is finished\")\n",
    "    \n",
    "    def prepare_interchange_mask(\n",
    "        self,\n",
    "        lengths, dual_lengths,\n",
    "        pred_mask, dual_pred_mask,\n",
    "    ):        \n",
    "        # params\n",
    "        interchange_prop = self.interchange_prop\n",
    "        interchange_max_token = self.interchange_max_token # if -1 then we don't restrict on this.\n",
    "        interchange_masked_token_only = self.interchange_masked_token_only\n",
    "        interchange_consecutive_only = self.interchange_consecutive_only\n",
    "        \n",
    "        interchange_mask = torch.zeros_like(pred_mask, dtype=torch.bool)\n",
    "        dual_interchange_mask = torch.zeros_like(dual_pred_mask, dtype=torch.bool)\n",
    "\n",
    "        batch_size, max_seq_len = pred_mask.shape[0], pred_mask.shape[1]\n",
    "        _, dual_max_seq_len = dual_pred_mask.shape[0], dual_pred_mask.shape[1]\n",
    "        interchange_position = []\n",
    "        for i in range(0, batch_size):\n",
    "            min_len = min(lengths[i].tolist(), dual_lengths[i].tolist())\n",
    "            if interchange_consecutive_only:\n",
    "                if interchange_max_token != -1:\n",
    "                    interchange_count = min(interchange_max_token, int(min_len*interchange_prop))\n",
    "                else:\n",
    "                    interchange_count = int(min_len*interchange_prop)\n",
    "                start_index = random.randint(0, lengths[i].tolist()-interchange_count)\n",
    "                end_index = start_index + interchange_count\n",
    "                dual_start_index = random.randint(0, dual_lengths[i].tolist()-interchange_count)\n",
    "                dual_end_index = dual_start_index + interchange_count\n",
    "                interchange_mask[i][start_index:end_index] = 1\n",
    "                dual_interchange_mask[i][dual_start_index:dual_end_index] = 1\n",
    "            else:\n",
    "                # we follow these steps to sample the position:\n",
    "                # 1. sample positions in the main example\n",
    "                # 2. get the actual sampled positions\n",
    "                # 3. sample accordingly from the dual example\n",
    "                if interchange_masked_token_only:\n",
    "                    # a corner case we need to consider is that the masked token\n",
    "                    # numbers may differ across two examples.\n",
    "                    interchange_count = pred_mask[i].sum()\n",
    "                    if interchange_count > dual_lengths[i]:\n",
    "                        # not likely, but we need to handle this.\n",
    "                        interchange_count = dual_lengths[i]\n",
    "                    interchange_position = pred_mask[i].nonzero().view(-1).tolist()\n",
    "                    interchange_position = random.sample(interchange_position, interchange_count)\n",
    "                    interchange_mask[i][interchange_position] = 1\n",
    "                    dual_interchange_position = random.sample(range(dual_max_seq_len), interchange_count)\n",
    "                    dual_interchange_mask[i][dual_interchange_position] = 1\n",
    "                else:\n",
    "                    if interchange_max_token != -1:\n",
    "                        interchange_count = min(interchange_max_token, int(min_len*interchange_prop))\n",
    "                    else:\n",
    "                        interchange_count = int(min_len*interchange_prop)\n",
    "                    interchange_position = random.sample(range(max_seq_len), interchange_count)\n",
    "                    interchange_mask[i][interchange_position] = 1\n",
    "                    dual_interchange_position = random.sample(range(dual_max_seq_len), interchange_count)\n",
    "                    dual_interchange_mask[i][dual_interchange_position] = 1\n",
    "\n",
    "        # sanity checks\n",
    "        assert interchange_mask.long().sum(dim=-1).tolist() == \\\n",
    "                dual_interchange_mask.long().sum(dim=-1).tolist()\n",
    "\n",
    "        return interchange_mask, dual_interchange_mask\n",
    "    \n",
    "    def step_diito(\n",
    "        self,\n",
    "        input_ids,\n",
    "        input_mask,\n",
    "        segment_ids,\n",
    "        label_ids,\n",
    "        dual_input_ids,\n",
    "        dual_input_mask,\n",
    "        dual_segment_ids,\n",
    "        dual_label_ids,\n",
    "    ):\n",
    "        interchange_mask, dual_interchange_mask = self.prepare_interchange_mask(\n",
    "            input_mask.sum(dim=-1), dual_input_mask.sum(dim=-1),\n",
    "            input_mask, dual_input_mask,\n",
    "        )\n",
    "        print(interchange_mask.sum(dim=-1))\n",
    "        print(dual_interchange_mask.sum(dim=-1))\n",
    "        # first, we simply prepare interchange positions.\n",
    "        pass\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        input_ids,\n",
    "        input_mask,\n",
    "        segment_ids,\n",
    "        label_ids,\n",
    "    ):\n",
    "        # teacher no_grad() forward pass.\n",
    "        with torch.no_grad():\n",
    "            if self.alpha == 0:\n",
    "                teacher_pred, teacher_patience = None, None\n",
    "            else:\n",
    "                # define a new function to compute loss values for both output_modes\n",
    "                full_output_teacher, pooled_output_teacher = self.teacher_encoder(\n",
    "                    input_ids, segment_ids, input_mask\n",
    "                )\n",
    "                if self.kd_model.lower() in['kd', 'kd.cls']:\n",
    "                    teacher_pred = self.teacher_classifier(pooled_output_teacher)\n",
    "                    if self.kd_model.lower() == 'kd.cls':\n",
    "                        teacher_patience = torch.stack(full_output_teacher[:-1]).transpose(0, 1)\n",
    "                        if self.fp16:\n",
    "                            teacher_patience = teacher_patience.half()\n",
    "                        layer_index = [int(i) for i in self.fc_layer_idx.split(',')]\n",
    "                        teacher_patience = torch.stack(\n",
    "                            [torch.FloatTensor(teacher_patience[:,int(i)]) for i in layer_index]\n",
    "                        ).transpose(0, 1)\n",
    "                    else:\n",
    "                        teacher_patience = None\n",
    "                else:\n",
    "                    raise ValueError(f'{self.kd_model} not implemented yet')\n",
    "                if self.fp16:\n",
    "                    teacher_pred = teacher_pred.half()\n",
    "            \n",
    "        # student with_grad() forward pass.\n",
    "        full_output_student, pooled_output_student = self.student_encoder(\n",
    "            input_ids, segment_ids, input_mask\n",
    "        )\n",
    "        if self.kd_model.lower() in['kd', 'kd.cls']:\n",
    "            logits_pred_student = self.student_classifier(\n",
    "                pooled_output_student\n",
    "            )\n",
    "            if self.kd_model.lower() == 'kd.cls':\n",
    "                student_patience = torch.stack(full_output_student[:-1]).transpose(0, 1)\n",
    "            else:\n",
    "                student_patience = None\n",
    "        else:\n",
    "            raise ValueError(f'{self.kd_model} not implemented yet')\n",
    "\n",
    "        # calculate loss\n",
    "        loss_dl, kd_loss, ce_loss = distillation_loss(\n",
    "            logits_pred_student, label_ids, teacher_pred, T=self.T, alpha=self.alpha\n",
    "        )\n",
    "        if self.beta > 0:\n",
    "            if student_patience.shape[0] != input_ids.shape[0]:\n",
    "                # For RACE\n",
    "                n_layer = student_patience.shape[1]\n",
    "                student_patience = student_patience.transpose(0, 1).contiguous().view(\n",
    "                    n_layer, input_ids.shape[0], -1\n",
    "                ).transpose(0,1)\n",
    "            pt_loss = self.beta * patience_loss(\n",
    "                teacher_patience, student_patience, \n",
    "                self.normalize_patience\n",
    "            )\n",
    "            loss = loss_dl + pt_loss\n",
    "        else:\n",
    "            pt_loss = torch.tensor(0.0)\n",
    "            loss = loss_dl\n",
    "        if self.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "        \n",
    "        # bookkeeping?\n",
    "        self.last_loss_dl = 0\n",
    "        self.last_kd_loss = 0\n",
    "        self.last_ce_loss = 0 \n",
    "        self.last_pt_loss = 0\n",
    "        \n",
    "        self.total_loss_epoch += loss.item()\n",
    "        self.last_loss = loss.item()\n",
    "        self.last_loss_dl = loss_dl.mean().item() if self.n_gpu > 0 else loss_dl.item()\n",
    "        self.last_kd_loss = kd_loss.mean().item() if self.n_gpu > 0 else kd_loss.item()\n",
    "        self.last_ce_loss = ce_loss.mean().item() if self.n_gpu > 0 else ce_loss.item()\n",
    "        self.last_pt_loss = pt_loss.mean().item() if self.n_gpu > 0 else pt_loss.item()\n",
    "        \n",
    "        n_sample = input_ids.shape[0]\n",
    "        self.acc_tr_loss += self.last_loss * n_sample\n",
    "        self.acc_tr_kd_loss += self.last_kd_loss * n_sample\n",
    "        self.acc_tr_ce_loss += self.last_ce_loss * n_sample\n",
    "        self.acc_tr_pt_loss = self.last_pt_loss * n_sample\n",
    "        pred_cls = logits_pred_student.data.max(1)[1]\n",
    "        self.acc_tr_acc += pred_cls.eq(label_ids).sum().cpu().item()\n",
    "        self.n_sequences_epoch += n_sample\n",
    "        \n",
    "        self.tr_loss = self.acc_tr_loss / self.n_sequences_epoch\n",
    "        self.tr_kd_loss = self.acc_tr_kd_loss / self.n_sequences_epoch\n",
    "        self.tr_ce_loss = self.acc_tr_ce_loss / self.n_sequences_epoch\n",
    "        self.tr_pt_loss = self.acc_tr_pt_loss / self.n_sequences_epoch\n",
    "        self.tr_acc = self.acc_tr_acc / self.n_sequences_epoch\n",
    "              \n",
    "        self.optimize(loss)\n",
    "            \n",
    "    def optimize(self, loss):\n",
    "        if self.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.gradient_accumulation_steps\n",
    "        \n",
    "        # backward()\n",
    "        if self.fp16:\n",
    "            self.optimizer.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        self.iter()\n",
    "\n",
    "        if self.n_iter % self.gradient_accumulation_steps == 0:\n",
    "            if self.fp16:\n",
    "                self.lr_this_step = self.learning_rate * warmup_linear(\n",
    "                    self.n_total_iter / self.num_train_optimization_steps,\n",
    "                    self.warmup_proportion\n",
    "                )\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.lr_this_step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "    \n",
    "    def iter(self):\n",
    "        \"\"\"\n",
    "        Update global counts, write to tensorboard and save checkpoint.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_iter += 1\n",
    "        self.n_total_iter += 1\n",
    "        if self.n_total_iter % self.params.checkpoint_interval == 0:\n",
    "            pass\n",
    "            # you can uncomment this line, if you really have checkpoints.\n",
    "            # self.save_checkpoint()\n",
    "        \n",
    "        \"\"\"\n",
    "        Logging is not affected by the flag skip_update_iter.\n",
    "        We want to log crossway effects, and losses should be\n",
    "        in the same magnitude.\n",
    "        \"\"\"\n",
    "        if self.n_total_iter % self.params.log_interval == 0:\n",
    "            self.log_tensorboard()\n",
    "            self.last_log = time.time()\n",
    "\n",
    "    def log_tensorboard(self):\n",
    "        \"\"\"\n",
    "        Log into tensorboard. Only by the master process.\n",
    "        \"\"\"\n",
    "\n",
    "        log_train = open(os.path.join(self.output_dir, 'train_log.txt'), 'a', buffering=1)\n",
    "        print('{},{},{},{},{},{},{},{}'.format(\n",
    "                self.epoch+1, self.n_total_iter, self.n_iter, \n",
    "                self.tr_acc,\n",
    "                self.tr_loss, \n",
    "                self.tr_kd_loss,\n",
    "                self.tr_ce_loss, \n",
    "                self.tr_pt_loss\n",
    "            ),\n",
    "            file=log_train\n",
    "        )\n",
    "        log_train.close()\n",
    "        \n",
    "        if not self.is_wandb:\n",
    "            pass # log to local logging file?\n",
    "        else:    \n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/loss\": self.last_loss, \n",
    "                    \"train/loss_dl\": self.last_loss_dl, \n",
    "                    \"train/kd_loss\": self.last_kd_loss, \n",
    "                    \"train/ce_loss\": self.last_ce_loss, \n",
    "                    \"train/pt_loss\": self.last_pt_loss, \n",
    "                    \n",
    "                    \"train/epoch_loss\": self.tr_loss, \n",
    "                    \"train/epoch_kd_loss\": self.tr_kd_loss, \n",
    "                    \"train/epoch_ce_loss\": self.tr_ce_loss, \n",
    "                    \"train/epoch_pt_loss\": self.tr_pt_loss, \n",
    "                    \"train/epoch_tr_acc\": self.tr_acc, \n",
    "                }, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/learning_rate\": self.lr_this_step,\n",
    "                    \"train/speed\": time.time() - self.last_log,\n",
    "                }, \n",
    "                step=self.n_total_iter\n",
    "            )\n",
    "\n",
    "    def end_epoch(self):\n",
    "        \"\"\"\n",
    "        Finally arrived at the end of epoch (full pass on dataset).\n",
    "        Do some tensorboard logging and checkpoint saving.\n",
    "        \"\"\"\n",
    "        logger.info(f\"{self.n_sequences_epoch} sequences have been trained during this epoch.\")\n",
    "\n",
    "        # let us do evaluation on the eval just for bookkeeping.\n",
    "        # make sure this is not intervening your training in anyway\n",
    "        # otherwise, data is leaking!\n",
    "        if 'race' in self.task_name:\n",
    "            result = eval_model_dataloader(\n",
    "                self.student_encoder, self.student_classifier, \n",
    "                self.eval_dataset, self.device, False\n",
    "            )\n",
    "        else:\n",
    "            result = eval_model_dataloader_nli(\n",
    "                self.task_name.lower(), self.eval_label_ids, \n",
    "                self.student_encoder, self.student_classifier, self.eval_dataset,\n",
    "                self.kd_model, self.num_labels, self.device, \n",
    "                self.weights, self.fc_layer_idx, self.output_mode\n",
    "            )\n",
    "        log_eval = open(os.path.join(self.output_dir, 'eval_log.txt'), 'a', buffering=1)\n",
    "        if self.task_name in ['CoLA']:\n",
    "            print('{},{},{}'.format(self.epoch+1, result['mcc'], result['eval_loss']), file=log_eval)\n",
    "        else:\n",
    "            if 'race' in self.task_name:\n",
    "                print('{},{},{}'.format(self.epoch+1, result['acc'], result['loss']), file=log_eval)\n",
    "            else:\n",
    "                print('{},{},{}'.format(self.epoch+1, result['acc'], result['eval_loss']), file=log_eval)\n",
    "        log_eval.close()\n",
    "        \n",
    "        self.save_checkpoint()\n",
    "        if self.is_wandb:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch/loss\": self.total_loss_epoch / self.n_iter, \n",
    "                    'epoch': self.epoch\n",
    "                }\n",
    "            )\n",
    "            if self.task_name in ['CoLA']:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch/eval_mcc\": result['mcc'], \n",
    "                        \"epoch/eval_loss\": result['eval_loss'], \n",
    "                        'epoch': self.epoch\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                if 'race' in self.task_name:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch/eval_acc\": result['acc'], \n",
    "                            \"epoch/eval_loss\": result['loss'], \n",
    "                            'epoch': self.epoch\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch/eval_acc\": result['acc'], \n",
    "                            \"epoch/eval_loss\": result['eval_loss'], \n",
    "                            'epoch': self.epoch\n",
    "                        }\n",
    "                    ) \n",
    "\n",
    "        self.epoch += 1\n",
    "        self.n_sequences_epoch = 0\n",
    "        self.n_iter = 0\n",
    "        self.total_loss_epoch = 0\n",
    "        \n",
    "        self.acc_tr_loss = 0\n",
    "        self.acc_tr_kd_loss = 0\n",
    "        self.acc_tr_ce_loss = 0\n",
    "        self.acc_tr_acc = 0\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_name=None):\n",
    "        if checkpoint_name == None:\n",
    "            if self.n_gpu > 1:\n",
    "                torch.save(\n",
    "                    self.student_encoder.module.state_dict(), \n",
    "                    os.path.join(self.output_dir, self.output_model_file + f'_e.{self.epoch}.encoder.pkl')\n",
    "                )\n",
    "                torch.save(\n",
    "                    self.student_classifier.module.state_dict(), \n",
    "                    os.path.join(self.output_dir, self.output_model_file + f'_e.{self.epoch}.cls.pkl')\n",
    "                )\n",
    "            else:\n",
    "                torch.save(\n",
    "                    self.student_encoder.state_dict(), \n",
    "                    os.path.join(self.output_dir, self.output_model_file + f'_e.{self.epoch}.encoder.pkl')\n",
    "                )\n",
    "                torch.save(\n",
    "                    self.student_classifier.state_dict(), \n",
    "                    os.path.join(self.output_dir, self.output_model_file + f'_e.{self.epoch}.cls.pkl')\n",
    "                )\n",
    "        else:\n",
    "            if self.n_gpu > 1:\n",
    "                torch.save(\n",
    "                    self.student_encoder.module.state_dict(), \n",
    "                    os.path.join(self.output_dir, \"encoder.\"+checkpoint_name)\n",
    "                )\n",
    "                torch.save(\n",
    "                    self.student_classifier.module.state_dict(), \n",
    "                    os.path.join(self.output_dir, \"cls.\"+checkpoint_name)\n",
    "                )\n",
    "            else:\n",
    "                torch.save(\n",
    "                    self.student_encoder.state_dict(), \n",
    "                    os.path.join(self.output_dir, \"encoder.\"+checkpoint_name)\n",
    "                )\n",
    "                torch.save(\n",
    "                    self.student_classifier.state_dict(), \n",
    "                    os.path.join(self.output_dir, \"cls.\"+checkpoint_name)\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:43:22 - INFO - __main__ -   Initializing Normal Distiller (Task Specific)\n",
      "02/11/2022 01:43:22 - INFO - __main__ -   FP16 is not activated, use BertAdam\n"
     ]
    }
   ],
   "source": [
    "distiller = TaskSpecificCausalDistiller(\n",
    "    args, \n",
    "    train_dataloader, eval_dataloader, \n",
    "    eval_label_ids, num_labels, output_mode,\n",
    "    student_encoder, student_classifier,\n",
    "    teacher_encoder, teacher_classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.is_diito = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2022 01:43:23 - INFO - __main__ -   Hey Zen: Let's go get some drinks.\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "-Iter:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 3, 3, 2, 2, 1, 1, 2, 1, 3, 1, 5, 2, 3,\n",
      "        2, 3, 4, 1, 1, 3, 1, 1])\n",
      "tensor([3, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 3, 3, 2, 2, 1, 1, 2, 1, 3, 1, 5, 2, 3,\n",
      "        2, 3, 4, 1, 1, 3, 1, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c95bf6881d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hey Zen: Let's go get some drinks.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdistiller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-34ae18efda9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     {\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34m\"Last_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{self.last_loss:.2f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                         \u001b[0;34m\"Avg_cum_loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{self.total_loss_epoch/self.n_iter:.2f}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                     }\n\u001b[1;32m    232\u001b[0m                 )\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "logger.info(\"Hey Zen: Let's go get some drinks.\")\n",
    "distiller.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_eval:\n",
    "    # Save a trained model and the associated configuration\n",
    "    if 'race' in task_name:\n",
    "        result = eval_model_dataloader(student_encoder, student_classifier, eval_dataloader, device, False)\n",
    "    else:\n",
    "        result = eval_model_dataloader_nli(args.task_name.lower(), eval_label_ids, student_encoder, student_classifier, eval_dataloader,\n",
    "                                           args.kd_model, num_labels, device, args.weights, args.fc_layer_idx, output_mode)\n",
    "\n",
    "    output_test_file = os.path.join(args.output_dir, \"eval_results_\" + output_model_file + '.txt')\n",
    "    with open(output_test_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key]))) \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
