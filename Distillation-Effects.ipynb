{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating how our interchange intervention effect the output.\n",
    "Through some trials of experiments, we find that for the most of the time, the teacher model will be experience huge output change with the interchange interventions. We use this document to see what interchange intervention is effective (i.e., having salient effects on the outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BERT.pytorch_pretrained_bert.modeling import BertConfig\n",
    "from BERT.pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from BERT.pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from distiller import TaskSpecificDistiller\n",
    "from causal_distiller import TaskSpecificCausalDistiller\n",
    "\n",
    "from src.argument_parser import default_parser, get_predefine_argv, complete_argument\n",
    "from src.nli_data_processing import processors, output_modes\n",
    "from src.data_processing import init_model, get_task_dataloader\n",
    "from src.modeling import BertForSequenceClassificationEncoder, FCClassifierForSequenceClassification, FullFCClassifierForSequenceClassification\n",
    "from src.utils import load_model, count_parameters, eval_model_dataloader_nli, eval_model_dataloader\n",
    "from src.KD_loss import distillation_loss, patience_loss, diito_distillation_loss\n",
    "from envs import HOME_DATA_FOLDER\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# Prepare Parser\n",
    "##########################################################################\n",
    "parser = default_parser()\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    logger.info(\"IN DEBUG MODE\")\n",
    "    # run Patient Teacher by uncommenting below cmd\n",
    "    argv = get_predefine_argv('glue', 'SST-2', 'kd.cls')\n",
    "    try:\n",
    "        args = parser.parse_args(argv)\n",
    "    except NameError:\n",
    "        raise ValueError('please uncomment one of option above to start training')\n",
    "    args.max_training_examples = 1000\n",
    "else:\n",
    "    logger.info(\"IN CMD MODE\")\n",
    "    args = parser.parse_args()\n",
    "args = complete_argument(args, is_debug=DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Datasets and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.raw_data_dir = os.path.join(HOME_DATA_FOLDER, 'data_raw', args.task_name)\n",
    "args.feat_data_dir = os.path.join(HOME_DATA_FOLDER, 'data_feat', args.task_name)\n",
    "\n",
    "args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "logger.info('actual batch size on all GPU = %d' % args.train_batch_size)\n",
    "device, n_gpu = args.device, args.n_gpu\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "logger.info('Input Argument Information')\n",
    "args_dict = vars(args)\n",
    "for a in args_dict:\n",
    "    logger.info('%-28s  %s' % (a, args_dict[a]))\n",
    "\n",
    "#########################################################################\n",
    "# Prepare  Data\n",
    "##########################################################################\n",
    "task_name = args.task_name.lower()\n",
    "\n",
    "if task_name not in processors and 'race' not in task_name:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "if 'race' in task_name:\n",
    "    pass\n",
    "else:\n",
    "    processor = processors[task_name]()\n",
    "    output_mode = output_modes[task_name]\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n",
    "\n",
    "if args.do_train:\n",
    "    train_sampler = SequentialSampler if DEBUG else RandomSampler\n",
    "    read_set = 'train'\n",
    "    logger.info('skipping loading teacher\\'s predictoin, we calculate this on-the-fly')\n",
    "    train_examples, train_dataloader, _ = get_task_dataloader(task_name, read_set, tokenizer, args, SequentialSampler,\n",
    "                                                              batch_size=args.train_batch_size)\n",
    "    num_train_optimization_steps = math.ceil(len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    args.num_train_optimization_steps = num_train_optimization_steps\n",
    "\n",
    "    # Run prediction for full data\n",
    "    eval_examples, eval_dataloader, eval_label_ids = get_task_dataloader(task_name, 'dev', tokenizer, args, SequentialSampler, batch_size=args.eval_batch_size)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "#########################################################################\n",
    "# Prepare model\n",
    "#########################################################################\n",
    "student_config = BertConfig(os.path.join(args.bert_model, 'bert_config.json'))\n",
    "teacher_config = BertConfig(os.path.join(args.bert_model, 'bert_config.json'))\n",
    "if args.kd_model.lower() in ['kd', 'kd.cls']:\n",
    "    logger.info('using normal Knowledge Distillation')\n",
    "    output_all_layers = args.kd_model.lower() == 'kd.cls'\n",
    "    logger.info('*' * 77)\n",
    "    logger.info(\"Loading the student model...\")\n",
    "    logger.info('*' * 77)\n",
    "    student_encoder, student_classifier = init_model(\n",
    "        task_name, output_all_layers, \n",
    "        args.student_hidden_layers, student_config,\n",
    "    )\n",
    "\n",
    "    n_student_layer = len(student_encoder.bert.encoder.layer)\n",
    "    student_encoder = load_model(\n",
    "        student_encoder, args.encoder_checkpoint_student, args, 'student', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    logger.info('*' * 77)\n",
    "    student_classifier = load_model(\n",
    "        student_classifier, args.cls_checkpoint_student, args, 'classifier', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    \n",
    "    logger.info('*' * 77)\n",
    "    logger.info(\"Loading the teacher model...\")\n",
    "    logger.info('*' * 77)\n",
    "    # since we also calculate teacher's output on-fly, we need to load the teacher model as well.\n",
    "    # note that, we assume teacher model is pre-trained already.\n",
    "    teacher_encoder, teacher_classifier = init_model(\n",
    "        task_name, output_all_layers, \n",
    "        teacher_config.num_hidden_layers, teacher_config,\n",
    "    )\n",
    "    \n",
    "    n_teacher_layer = len(teacher_encoder.bert.encoder.layer)\n",
    "    teacher_encoder = load_model(\n",
    "        teacher_encoder, args.encoder_checkpoint_teacher, args, 'student', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "    logger.info('*' * 77)\n",
    "    teacher_classifier = load_model(\n",
    "        teacher_classifier, args.cls_checkpoint_teacher, args, 'classifier', \n",
    "        verbose=True, DEBUG=False,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # originally, the codebase supports kd.full, but that is never used.\n",
    "    raise ValueError('%s KD not found, please use kd or kd.cls' % args.kd)\n",
    "\n",
    "n_param_student = count_parameters(student_encoder) + count_parameters(student_classifier)\n",
    "logger.info('number of layers in student model = %d' % n_student_layer)\n",
    "logger.info('num parameters in student model are %d and %d' % (count_parameters(student_encoder), count_parameters(student_classifier))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/21/2022 14:03:55 - WARNING - causal_distiller -   training flags:\n",
      "02/21/2022 14:03:55 - WARNING - causal_distiller -   is_diito=False\n",
      "02/21/2022 14:03:55 - WARNING - causal_distiller -   data_augment=False\n",
      "02/21/2022 14:03:55 - WARNING - causal_distiller -   data_pair=False\n"
     ]
    }
   ],
   "source": [
    "distiller = TaskSpecificCausalDistiller(\n",
    "    args, \n",
    "    train_dataloader, eval_dataloader, \n",
    "    eval_label_ids, num_labels, output_mode,\n",
    "    student_encoder, student_classifier,\n",
    "    teacher_encoder, teacher_classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
