{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating how our interchange intervention effect the output.\n",
    "Through some trials of experiments, we find that for the most of the time, the teacher model will be experience huge output change with the interchange interventions. We use this document to see what interchange intervention is effective (i.e., having salient effects on the outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import psutil\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import BatchSampler, DataLoader, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from BERT.pytorch_pretrained_bert.modeling import BertConfig\n",
    "from BERT.pytorch_pretrained_bert.optimization import BertAdam, warmup_linear\n",
    "from BERT.pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "from distiller import TaskSpecificDistiller\n",
    "from causal_distiller import TaskSpecificCausalDistiller\n",
    "\n",
    "from src.argument_parser import default_parser, get_predefine_argv, complete_argument\n",
    "from src.nli_data_processing import processors, output_modes\n",
    "from src.data_processing import init_model, get_task_dataloader\n",
    "from src.modeling import BertForSequenceClassificationEncoder, FCClassifierForSequenceClassification, FullFCClassifierForSequenceClassification\n",
    "from src.utils import load_model, count_parameters, eval_model_dataloader_nli, eval_model_dataloader\n",
    "from src.KD_loss import distillation_loss, patience_loss, diito_distillation_loss\n",
    "from envs import HOME_DATA_FOLDER\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mock Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/21/2022 13:06:32 - INFO - __main__ -   IN DEBUG MODE\n",
      "usage: ipykernel_launcher.py [-h] [--task_name TASK_NAME]\n",
      "                             [--run_name RUN_NAME] [--output_dir OUTPUT_DIR]\n",
      "                             [--log_every_step LOG_EVERY_STEP]\n",
      "                             [--log_interval LOG_INTERVAL]\n",
      "                             [--checkpoint_interval CHECKPOINT_INTERVAL]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                             [--max_training_examples MAX_TRAINING_EXAMPLES]\n",
      "                             [--seed SEED]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--fp16 FP16] [--loss_scale LOSS_SCALE]\n",
      "                             [--student_hidden_layers STUDENT_HIDDEN_LAYERS]\n",
      "                             [--teacher_prediction TEACHER_PREDICTION]\n",
      "                             [--warmup_proportion WARMUP_PROPORTION]\n",
      "                             [--bert_model BERT_MODEL]\n",
      "                             [--encoder_checkpoint_student ENCODER_CHECKPOINT_STUDENT]\n",
      "                             [--cls_checkpoint_student CLS_CHECKPOINT_STUDENT]\n",
      "                             [--encoder_checkpoint_teacher ENCODER_CHECKPOINT_TEACHER]\n",
      "                             [--cls_checkpoint_teacher CLS_CHECKPOINT_TEACHER]\n",
      "                             [--output_all_encoded_layers OUTPUT_ALL_ENCODED_LAYERS]\n",
      "                             [--alpha ALPHA] [--T T] [--beta BETA]\n",
      "                             [--kd_model KD_MODEL]\n",
      "                             [--fc_layer_idx FC_LAYER_IDX] [--weights WEIGHTS]\n",
      "                             [--normalize_patience NORMALIZE_PATIENCE]\n",
      "                             [--do_train] [--do_eval] [--is_diito]\n",
      "                             [--diito_type DIITO_TYPE]\n",
      "                             [--neuron_mapping NEURON_MAPPING]\n",
      "                             [--interchange_prop INTERCHANGE_PROP]\n",
      "                             [--interchange_max_token INTERCHANGE_MAX_TOKEN]\n",
      "                             [--interchange_consecutive_only] [--data_augment]\n",
      "                             [--data_pair] [--is_wandb]\n",
      "                             [--wandb_metadata WANDB_METADATA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: True True\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# Prepare Parser\n",
    "##########################################################################\n",
    "parser = default_parser()\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    logger.info(\"IN DEBUG MODE\")\n",
    "    # run simple fune-tuning *teacher* by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'finetune_teacher')\n",
    "\n",
    "    # run simple fune-tuning *student* by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'finetune_student')\n",
    "\n",
    "    # run vanilla KD by uncommenting below cmd\n",
    "    # argv = get_predefine_argv('glue', 'RTE', 'kd')\n",
    "\n",
    "    # run Patient Teacher by uncommenting below cmd\n",
    "    argv = get_predefine_argv('glue', 'SST-2', 'kd.cls')\n",
    "    try:\n",
    "        args = parser.parse_args(argv)\n",
    "    except NameError:\n",
    "        raise ValueError('please uncomment one of option above to start training')\n",
    "    args.max_training_examples = 1000\n",
    "else:\n",
    "    logger.info(\"IN CMD MODE\")\n",
    "    args = parser.parse_args()\n",
    "args = complete_argument(args, is_debug=DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
